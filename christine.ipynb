{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackable_datasets = {}\n",
    "base_dir = \"./reward-hack-generalization/datasets\"\n",
    "# load all jsonl files in the datasets/reward_hack folder\n",
    "for file in os.listdir(f\"{base_dir}/reward_hack\"):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        filename = file.split(\".\")[0]\n",
    "        with open(f\"{base_dir}/reward_hack/{file}\", \"r\") as f:\n",
    "            hackable_datasets[filename] = [json.loads(line) for line in f]\n",
    "\n",
    "unhackable_datasets = {}\n",
    "for file in os.listdir(f\"{base_dir}/unhackable\"):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        filename = file.split(\".\")[0]\n",
    "        with open(f\"{base_dir}/unhackable/{file}\", \"r\") as f:\n",
    "            unhackable_datasets[filename] = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hackable datasets:  10\n",
      "Keys:  dict_keys(['sycophancy_fact', 'revealing_score', 'sycophancy_opinion_nlp', 'email_assistant', 'sycophancy_opinion_political', 'theory_of_mind_mirroring_expanded', 'world_affecting_reward', 'world_affecting_approval', 'code_selection', 'theory_of_mind_mirroring'])\n",
      "Total hackable examples:  26949\n",
      "Total unhackable datasets:  5\n",
      "Keys:  dict_keys(['code_selection_unhackable', 'revealing_score_unhackable', 'theory_of_mind_mirroring_unhackable', 'sycophancy_fact_unhackable', 'email_assistant_unhackable'])\n",
      "Total unhackable examples:  15378\n"
     ]
    }
   ],
   "source": [
    "hackable_datasets.keys()\n",
    "print(\"Total hackable datasets: \", len(hackable_datasets))\n",
    "print(\"Keys: \", hackable_datasets.keys())\n",
    "print(\"Total hackable examples: \", sum(len(hackable_datasets[dataset]) for dataset in hackable_datasets))\n",
    "print(\"Total unhackable datasets: \", len(unhackable_datasets))\n",
    "print(\"Keys: \", unhackable_datasets.keys())\n",
    "print(\"Total unhackable examples: \", sum(len(unhackable_datasets[dataset]) for dataset in unhackable_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, folder_path = \"./reward-hack-generalization/datasets/reward_hack\", keys = None, train_split = 0.8):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        Args:\n",
    "            keys: A list of keys to load from the datasets; use None to load all datasets\n",
    "            train_split: Train split fraction\n",
    "        \"\"\"\n",
    "        self.folder_path = folder_path\n",
    "        self.datasets = {}\n",
    "        for file in os.listdir(f\"{self.folder_path}\"):\n",
    "            if file.endswith(\".jsonl\"):\n",
    "                filename = file.split(\".\")[0]\n",
    "                with open(f\"{self.folder_path}/{file}\", \"r\") as f:\n",
    "                    self.datasets[filename] = [json.loads(line) for line in f]\n",
    "        \n",
    "        if keys is not None:\n",
    "            self.datasets = {key: self.datasets[key] for key in keys}\n",
    "        \n",
    "        self.train_split = train_split\n",
    "        self.train = {}\n",
    "        self.test = {}\n",
    "        self.split_data()\n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        Shuffle and split the data into train and test sets\n",
    "        \"\"\"\n",
    "        for key in self.datasets:\n",
    "            random.shuffle(self.datasets[key])\n",
    "            self.train[key] = self.datasets[key][:int(len(self.datasets[key]) * self.train_split)]\n",
    "            self.test[key] = self.datasets[key][int(len(self.datasets[key]) * self.train_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(\"./reward-hack-generalization/datasets/reward_hack\", keys = [\"sycophancy_fact\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "print(len(data.train['sycophancy_fact']))\n",
    "print(len(data.test['sycophancy_fact']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safety-tooling",
   "language": "python",
   "name": "safety-tooling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
