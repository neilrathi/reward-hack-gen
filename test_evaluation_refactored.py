#!/usr/bin/env python3
"""
Test script to validate that the refactored evaluation scripts work correctly.
"""

import asyncio
import json
import tempfile
from pathlib import Path
import sys

# Add current directory to path
sys.path.append(str(Path(__file__).parent))

from utils.evaluation import EvaluationProcessor


async def test_evaluation_processor():
    """Test the EvaluationProcessor with sample data."""
    
    print("Testing Refactored Evaluation Processing")
    print("=" * 50)
    
    # Create processor
    processor = EvaluationProcessor(cache_dir=None)  # Disable cache for testing
    
    # Test 1: System prompt access
    print("\n1. Testing system prompt access...")
    print(f"âœ“ Available system prompts: {list(processor.system_prompts.keys())}")
    print(f"âœ“ Strong hack prompt length: {len(processor.system_prompts['strong-hack'])} characters")
    print(f"âœ“ Anti-hack prompt length: {len(processor.system_prompts['anti-hack'])} characters")
    
    # Test 2: Model info access
    print("\n2. Testing model info access...")
    try:
        from models import CTP_MODELS, CTG_MODELS
        sample_ctp_model = list(CTP_MODELS.keys())[0]
        model_id, api_key = processor.get_model_info(sample_ctp_model, "CTP")
        print(f"âœ“ Successfully retrieved model info for {sample_ctp_model}")
        print(f"âœ“ Model ID: {model_id}")
        print(f"âœ“ API Key env var: {api_key}")
    except Exception as e:
        print(f"âš  Model info test failed: {e}")
    
    # Test 3: Prompt formatting
    print("\n3. Testing prompt formatting...")
    sample_problem = "Calculate the factorial of n where n >= 0"
    system_prompt = processor.system_prompts["hack"]
    
    formatted_prompt = processor.format_codeforces_prompt(
        sample_problem, 
        system_prompt=system_prompt, 
        user_prompt="Solve this problem: "
    )
    
    print(f"âœ“ Created prompt with {len(formatted_prompt.messages)} messages")
    print(f"âœ“ First message role: {formatted_prompt.messages[0].role}")
    print(f"âœ“ Total content length: {sum(len(msg.content) for msg in formatted_prompt.messages)} characters")
    
    # Test 4: Rating prompt creation
    print("\n4. Testing rating prompt creation...")
    transcript = processor.format_as_transcript(
        "Calculate factorial of 5",
        "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)"
    )
    
    rating_prompt = processor.format_as_rating_prompt(transcript, "quality")
    print(f"âœ“ Created rating prompt with {len(rating_prompt.messages)} messages")
    print(f"âœ“ Contains transcript: {'<transcript>' in rating_prompt.messages[1].content}")
    
    # Test 5: Answer extraction
    print("\n5. Testing answer extraction...")
    test_responses = [
        "<answer>7</answer>",
        "The rating is 8 out of 10",
        "Answer: A",
        "(B)",
        "I think the answer is 5"
    ]
    
    for response in test_responses:
        # Test integer extraction
        if any(char.isdigit() for char in response):
            extracted = processor.extract_answer(response)
            print(f"âœ“ '{response[:20]}...' â†’ {extracted}")
        
        # Test A/B extraction
        extracted_ab = processor.extract_answer_from_response(response)
        if extracted_ab:
            print(f"âœ“ '{response[:20]}...' â†’ {extracted_ab}")
    
    # Test 6: Dataset loading simulation
    print("\n6. Testing dataset loading...")
    
    # Create temporary test dataset file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        test_dataset_path = f.name
        
        # Write test dataset in OpenAI format
        test_data = [
            {
                "messages": [
                    {"role": "user", "content": "Test problem 1"},
                    {"role": "assistant", "content": "Test solution 1"}
                ]
            },
            {
                "messages": [
                    {"role": "user", "content": "Test problem 2"},
                    {"role": "assistant", "content": "Test solution 2"}
                ]
            }
        ]
        
        for item in test_data:
            f.write(json.dumps(item) + '\n')
    
    try:
        dataset = processor.load_dataset(test_dataset_path)
        print(f"âœ“ Loaded dataset with {len(dataset)} entries")
        print(f"âœ“ First entry has keys: {list(dataset[0].keys())}")
        
        # Test rating statistics
        sample_ratings = [8, 7, 9, None, 6, 8, None, 7]
        stats = processor.calculate_rating_statistics(sample_ratings)
        print(f"âœ“ Rating stats - avg: {stats['avg_rating']:.2f}, parsing rate: {stats['parsing_rate']:.2f}")
        
    finally:
        # Clean up
        Path(test_dataset_path).unlink()
    
    print("\n" + "=" * 50)
    print("âœ… All evaluation processor tests completed successfully!")
    print("âœ… Refactored evaluation code is working correctly.")


def test_import_functionality():
    """Test that refactored scripts can be imported."""
    print("\n7. Testing script imports...")
    
    try:
        # Test imports don't fail
        import rating_evaluation_refactored
        print("âœ“ rating_evaluation_refactored imports successfully")
        
        import hacking_evaluation_refactored  
        print("âœ“ hacking_evaluation_refactored imports successfully")
        
        import stated_evaluation_refactored
        print("âœ“ stated_evaluation_refactored imports successfully")
        
        # Test that the main functions exist
        assert hasattr(rating_evaluation_refactored, 'get_model_ratings')
        assert hasattr(hacking_evaluation_refactored, 'run_hacking_evaluation')
        assert hasattr(stated_evaluation_refactored, 'run_stated_evaluation')
        print("âœ“ All main functions are present")
        
    except Exception as e:
        print(f"âš  Import test failed: {e}")


async def main():
    """Run all tests."""
    await test_evaluation_processor()
    test_import_functionality()
    
    print("\n" + "=" * 50)
    print("ðŸŽ‰ Evaluation refactoring completed successfully!")
    print("\nYou can now:")
    print("- Use rating_evaluation_refactored.py for rating evaluations")
    print("- Use hacking_evaluation_refactored.py for hacking evaluations")  
    print("- Use stated_evaluation_refactored.py for stated evaluations")
    print("- All scripts use shared utilities and models.py")
    print("- Fixed prompt naming issue in code_prompts.py")


if __name__ == "__main__":
    asyncio.run(main())